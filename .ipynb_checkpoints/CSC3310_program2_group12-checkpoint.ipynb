{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Christian Basso, Konrad Rozpadek, Kyle Robinson\n",
    "\n",
    "Program 2: Benchmarking Insertion and Selection Sorts\n",
    "\n",
    "CSC3310 - 001\n",
    "\n",
    "Febuary 16, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program 2: Benchmarking Insertion and Selection Sorts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program impliments and benchmarks insertion and selection sorts. First, an implimentation of each algorithm is programmed, then tested with various array sizes and stuctures. Each algorithim is then benchmarked using a custom benchmark function. Each algorithim is benchmarked using 5 array sizes with each having three stucture variations (best, worst, and average case). A linear regression model is then made by taking the log of the run time and list size to determine the time complexity of each algorithim in relation to each other. These models are then plotted by algorithm and by array case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implimentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection(a):\n",
    "    for j in range(0, len(a) - 1):\n",
    "        smallest = j\n",
    "        for i in range(j + 1, len(a)):\n",
    "            if a[i] < a[smallest]:\n",
    "                smallest = i\n",
    "\n",
    "        temp = a[j]\n",
    "        a[j] = a[smallest]\n",
    "        a[smallest] = temp\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insertion Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertion(a):\n",
    "    for j in range(len(a)):\n",
    "        key = a[j]\n",
    "        i = j - 1\n",
    "        while i >= 0 and a[i] > key:\n",
    "            a[i + 1] = a[i]\n",
    "            i = i - 1\n",
    "        a[i + 1] = key\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Case Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_test = ([],[])\n",
    "keep_sorted_test = (list(range(100)), list(range(100)))\n",
    "inverse_test = (list(range(100, 0, -1)), sorted(list(range(100, 0, -1))))\n",
    "single_element_test = [[3], [3]]\n",
    "def create_random_array(num):\n",
    "    import random\n",
    "    a = []\n",
    "    for i in range(num):\n",
    "        a.append(random.randint(-1e9, 1e9))\n",
    "    return a\n",
    "rand_10 = create_random_array(10)\n",
    "rand_100 = create_random_array(100)\n",
    "rand_1000 = create_random_array(1000)\n",
    "random_10_test = (rand_10, sorted(rand_10))\n",
    "random_100_test = (rand_100, sorted(rand_100))\n",
    "random_1000_test = (rand_1000, sorted(rand_1000))\n",
    "tests = [empty_test, keep_sorted_test, inverse_test, \n",
    "         single_element_test, random_10_test, random_100_test, random_1000_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in tests:\n",
    "    before_sort = test[0]\n",
    "    expected = test[1]\n",
    "    selection_results = selection(before_sort)\n",
    "    insertion_results = insertion(before_sort)\n",
    "    assert expected == selection_results\n",
    "    assert expected == insertion_results\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to Create Benchmarking Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forwards Sorted (Best Case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_forwards_array(num):\n",
    "    a = []\n",
    "    for i in range(0, num):\n",
    "        a.append(i)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backwards Sorted (Worst Case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_backwards_array(num):\n",
    "    a = []\n",
    "    for i in range(num, 0, -1):\n",
    "        a.append(i)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomly Sorted (Average Case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_array(num):\n",
    "    import random\n",
    "    a = []\n",
    "    for i in range(num):\n",
    "        a.append(random.randint(0, num))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Benchmarking Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forwards_20 = create_forwards_array(20)\n",
    "forwards_200 = create_forwards_array(200)\n",
    "forwards_1000 = create_forwards_array(1000)\n",
    "forwards_5000 = create_forwards_array(5000)\n",
    "forwards_10000 = create_forwards_array(10000)\n",
    "\n",
    "backwards_20 = create_backwards_array(20)\n",
    "backwards_200 = create_backwards_array(200)\n",
    "backwards_1000 = create_backwards_array(1000)\n",
    "backwards_5000 = create_backwards_array(5000)\n",
    "backwards_10000 = create_backwards_array(10000)\n",
    "\n",
    "average_20 = create_random_array(20)\n",
    "average_200 = create_random_array(200)\n",
    "average_1000 = create_random_array(1000)\n",
    "average_5000 = create_random_array(5000)\n",
    "average_10000 = create_random_array(10000)\n",
    "list_sizes = [20,200,1000,5000,10000]\n",
    "fowards = [forwards_20,forwards_200,forwards_1000,forwards_5000,forwards_10000]\n",
    "backwards = [backwards_20,backwards_200,backwards_1000,backwards_5000,backwards_10000]\n",
    "average = [average_20,average_200,average_1000,average_5000,average_10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking Function\n",
    "As input, the function takes both a sorting function and the list to sort. It returns the elapsed time in seconds.\n",
    "\n",
    "This function creates a copy of the input to ensure that the original list is not modified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "def benchmark(algorithm, arr):\n",
    "    copy_arr = copy.copy(arr)\n",
    "    start_time = time.perf_counter()\n",
    "    algorithm(copy_arr)\n",
    "    end_time = time.perf_counter()\n",
    "    return end_time - start_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_fowards_selection = []\n",
    "time_backwards_selection = []\n",
    "time_average_selection = []\n",
    "time_fowards_insertion = []\n",
    "time_backwards_insertion = []\n",
    "time_average_insertion = []\n",
    "for arr in fowards:\n",
    "    time_fowards_selection.append(benchmark(selection, arr))\n",
    "    time_fowards_insertion.append(benchmark(insertion, arr))\n",
    "for arr in backwards:\n",
    "    time_backwards_selection.append(benchmark(selection, arr))\n",
    "    time_backwards_insertion.append(benchmark(insertion, arr))\n",
    "for arr in average:\n",
    "    time_average_selection.append(benchmark(selection, arr))\n",
    "    time_average_insertion.append(benchmark(insertion, arr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_selection_best = linregress (np.log(list_sizes) , np.log(time_fowards_selection) )\n",
    "m_insertion_best = linregress (np.log(list_sizes) , np.log(time_fowards_insertion) )\n",
    "m_selection_worst = linregress (np.log(list_sizes) , np.log(time_backwards_selection) )\n",
    "m_insertion_worst = linregress (np.log(list_sizes) , np.log(time_backwards_insertion) )\n",
    "m_selection_avg = linregress (np.log(list_sizes) , np.log(time_average_selection) )\n",
    "m_insertion_avg = linregress (np.log(list_sizes) , np.log(time_average_insertion) )\n",
    "print(\"Selection Best Case Model Slope: \", m_selection_best[0])\n",
    "print(\"Insertion Best Case Model Slope: \", m_insertion_best[0])\n",
    "print(\"Selection Worst Case Model Slope: \", m_selection_worst[0])\n",
    "print(\"Insertion Worst Case Model Slope: \", m_insertion_worst[0])\n",
    "print(\"Selection Average Case Model Slope: \", m_selection_avg[0])\n",
    "print(\"Insertion Average Case Model Slope: \", m_insertion_avg[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list_sizes, time_fowards_selection, label=\"best\")    \n",
    "plt.plot(list_sizes, time_backwards_selection, label=\"worst\")    \n",
    "plt.plot(list_sizes, time_average_selection, label=\"average\")    \n",
    "plt.xlabel(\"List Sizes\", fontsize=18)\n",
    "plt.ylabel(\"Run Times\", fontsize=18)\n",
    "plt.title(\"Selection Sort\", fontsize=20)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list_sizes, time_fowards_insertion, label=\"best\")    \n",
    "plt.plot(list_sizes, time_backwards_insertion, label=\"worst\")    \n",
    "plt.plot(list_sizes, time_average_insertion, label=\"average\")    \n",
    "plt.xlabel(\"List Sizes\", fontsize=18)\n",
    "plt.ylabel(\"Run Times\", fontsize=18)\n",
    "plt.title(\"Insertion Sort\", fontsize=20)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting best case\n",
    "plt.plot(list_sizes, time_fowards_selection, label=\"Selection Sort - Best Case\")\n",
    "plt.plot(list_sizes, time_fowards_insertion, label=\"Insertion Sort - Best Case\")\n",
    "plt.xlabel(\"List Sizes\", fontsize=12)\n",
    "plt.ylabel(\"Run Times\", fontsize=12)\n",
    "plt.title(\"Comparison of Run Times - Best Case\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting worst case\n",
    "plt.plot(list_sizes, time_backwards_selection, label=\"Selection Sort - Worst Case\")\n",
    "plt.plot(list_sizes, time_backwards_insertion, label=\"Insertion Sort - Worst Case\")\n",
    "plt.xlabel(\"List Sizes\", fontsize=12)\n",
    "plt.ylabel(\"Run Times\", fontsize=12)\n",
    "plt.title(\"Comparison of Run Times - Worst Case\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting average case\n",
    "plt.plot(list_sizes, time_average_insertion, label=\"Selection Sort - Avg Case\")\n",
    "plt.plot(list_sizes, time_average_selection, label=\"Insertion Sort - Avg Case\")\n",
    "plt.xlabel(\"List Sizes\", fontsize=12)\n",
    "plt.ylabel(\"Run Times\", fontsize=12)\n",
    "plt.title(\"Comparison of Run Times - Average Case\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a table of the theoretical and estimated run time functions for the 6 combinations (2 algorithms, 3 cases). Do your estimates match the theory? If not, you may have made a mistake somewhere.\n",
    "\n",
    "- Which algorithm had a better run time than the other and for which case(s)? Why do you think that one case was substantially faster for that algorithm? (Hint: focus on the inner loops.)\n",
    "\n",
    "Selection sort has consistent run times for best, worst, and average cases. Insertion sort shows variation between the different cases. For best case, insertion sort performs better than selection sort due to the fact that insertion sort can finish in linear time while selection sort still requires quadratic time. For worst case, insertion sort performs worse than selection sort since insertion sort involves more shifting of elements than selection, which only requires swapping. For average case, both algorithms tend to perform similarly, with both having quadratic time complexity, however selection sort can be slightly faster due to having fewer slightly fewer operations it needs to complete in a partially sorted array.\n",
    "\n",
    "- Based on your results, which of the two sorting algorithms would you use in practice? Why?\n",
    "\n",
    "In practice, both algorithms would be considered based on the known structure of the input. If the input is known to be mostly sorted, insertion sort would be the best choice. If the input is known to be mostly unsorted, selection sort would be the best choice. However, if the input is unknown, either algorithm would be a valid choice since they had nearly identical average case runtimes according to our results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
